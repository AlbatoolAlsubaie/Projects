{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47619532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d9f9380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af5fad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7f3fe",
   "metadata": {},
   "source": [
    "# Gether Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdabf1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter-archive-enhanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a02b09b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ProxyError",
     "evalue": "HTTPSConnectionPool(host='d17h27t6h515a5.cloudfront.net', port=443): Max retries exceeded with url: /topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002C8C5947640>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             conn = connection.create_connection(\n\u001b[0m\u001b[0;32m    175\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    917\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    695\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_new_proxy_conn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhttp_tunnel_required\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_proxy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_prepare_proxy\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m         \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[1;31m# Add certificate verification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSocketError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m             raise NewConnectionError(\n\u001b[0m\u001b[0;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000002C8C5947640>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m             retries = retries.increment(\n\u001b[0m\u001b[0;32m    756\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='d17h27t6h515a5.cloudfront.net', port=443): Max retries exceeded with url: /topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002C8C5947640>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\AALSUB~1\\AppData\\Local\\Temp/ipykernel_66644/568944352.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimage_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'image_predictions .tsv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \"\"\"\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ProxyError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mProxyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_SSLError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProxyError\u001b[0m: HTTPSConnectionPool(host='d17h27t6h515a5.cloudfront.net', port=443): Max retries exceeded with url: /topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002C8C5947640>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')))"
     ]
    }
   ],
   "source": [
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "image_url = requests.get(url, allow_redirects=True)\n",
    "\n",
    "with open('image_predictions .tsv', mode='wb')as file : \n",
    "    file.write(image_url.content)\n",
    "    df2 = pd.read_csv('image-predictions .tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187129a3",
   "metadata": {},
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "Query Twitter API for each tweet in the Twitter archive and save JSON in a text file\n",
    "#These are hidden to comply with Twitter's API terms and conditions\n",
    "consumer_key = 'HIDDEN'\n",
    "consumer_secret = 'HIDDEN'\n",
    "access_token = 'HIDDEN'\n",
    "access_secret = 'HIDDEN'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "#NOTE TO STUDENT WITH MOBILE VERIFICATION ISSUES:\n",
    "#df_1 is a DataFrame with the twitter_archive_enhanced.csv file. You may have to\n",
    "#change line 17 to match the name of your DataFrame with twitter_archive_enhanced.csv\n",
    "#NOTE TO REVIEWER: this student had mobile verification issues so the following\n",
    "#Twitter API code was sent to this student from a Udacity instructor\n",
    "#Tweet IDs for which to gather additional data via Twitter's API\n",
    "tweet_ids = df_1.tweet_id.values\n",
    "len(tweet_ids)\n",
    "\n",
    "#Query Twitter's API for JSON data for each tweet ID in the Twitter archive\n",
    "count = 0\n",
    "fails_dict = {}\n",
    "start = timer()\n",
    "#Save each tweet's returned JSON as a new line in a .txt file\n",
    "with open('tweet_json.txt', 'w') as outfile:\n",
    "    # This loop will likely take 20-30 minutes to run because of Twitter's rate limit\n",
    "    for tweet_id in tweet_ids:\n",
    "        count += 1\n",
    "        print(str(count) + \": \" + str(tweet_id))\n",
    "        try:\n",
    "            tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "            print(\"Success\")\n",
    "            json.dump(tweet._json, outfile)\n",
    "            outfile.write('\\n')\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Fail\")\n",
    "            fails_dict[tweet_id] = e\n",
    "            pass\n",
    "end = timer()\n",
    "print(end - start)\n",
    "print(fails_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f259b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_json('tweet-json.txt', lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aaa74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb538aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098acccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51227d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a9dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516981e3",
   "metadata": {},
   "source": [
    "# Data Cleaning - Quality #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef081f14",
   "metadata": {},
   "source": [
    "## Define ##\n",
    "\n",
    "•\tI made a copy of each database to make sure that all the cleaning efforts are done in a separate data and the original is preserved.\n",
    "\n",
    "•\tThen, I noticed that the timestamp column is not easy to read because the date and time are merged together. I separated them into a date column and a time column using the datetime from Pandas.\n",
    "\n",
    "•\tAfter that, the database had three columns for date and time: date column, time column, and the original timestamp column. I dropped the timestamp column using the drop function \n",
    "\n",
    "•\tI renamed the id column in df3_clean to tweet_id using the rename function to match the other databases because we will be merging the columns later.\n",
    "\n",
    "•\tI removed duplicate images in jpg_url column using duplicated() and drop_duplicates.\n",
    "\n",
    "•\tI rounded up the decimals in confidence rate columns to 2 decimals\n",
    "\n",
    "•\tI noticed that the names of the predicted dog breed are separated using “_” instead of spaces, so I replaced the _ in the predicted names to spaces.\n",
    "\n",
    "•\tI removed retweets \n",
    "\n",
    "•\tI dropped replies columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18971527",
   "metadata": {},
   "source": [
    "## Code ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33724b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we first make a copy to clean for each database\n",
    "df_clean = df.copy()\n",
    "df2_clean = df2.copy()\n",
    "df3_clean = df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a7329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first data quality issue: timestamp column needs to be separated to date and time columns using pandas' datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ff4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['Dates'] = pd.to_datetime(df_clean['timestamp']).dt.date\n",
    "df_clean['Time'] = pd.to_datetime(df_clean['timestamp']).dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644db957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second data quality issue: the database now has duplicate columns, so we should drop the timestamp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c0df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop('timestamp', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d0381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1089c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#third data quality issue: rename the id column in df3_clean to tweet_id to match the other databases\n",
    "#because we will be merging the columns later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c209d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_clean.rename(columns={'id': 'tweet_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_clean.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aad017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fourth data quality issue: remove duplicate images in jpg_url column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb264513",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_clean['jpg_url'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f136f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_clean['jpg_url'].drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f59dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fifth data quality issue: round the decimals in confidence rate columns to 2 decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b86273",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_clean.round({\"p1_conf\":2, \"p2_conf\":2, \"p3_conf\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c545f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sixth data quality issue: replace the _ in the predicted names to spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e85c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_clean['p1'] = df2_clean['p1'].str.replace('_',' ')\n",
    "df2_clean['p2'] = df2_clean['p2'].str.replace('_',' ')\n",
    "df2_clean['p3'] = df2_clean['p3'].str.replace('_',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098872b3",
   "metadata": {},
   "source": [
    "## Test ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b26e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seventh data quality issue: remove retweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e270ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean[np.isnan(df_clean['retweeted_status_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f0756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.drop(['retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1eb698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eighth data quality issue: drop reply columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae0da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.drop(['in_reply_to_user_id','in_reply_to_status_id'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac4ad98",
   "metadata": {},
   "source": [
    "## Test ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410e5347",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_clean.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c8aa80",
   "metadata": {},
   "source": [
    "# Tidyness #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e7a70a",
   "metadata": {},
   "source": [
    "## Define ##\n",
    "\n",
    "•\tThe database had columns for different dog types. Since only one type can be selected, the others just have “None” values. To tidy the database, I merged the columns into one column called “dogs” and changed the values to reflect the selected breed without the “None” values. Then, I dropped the original columns.\n",
    "\n",
    "•\tThen, I merged all three databases using “tweet_id” column to have one master database that contains all the columns. Note that I merged the clean databases and not the original.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2491fafa",
   "metadata": {},
   "source": [
    "## Code ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ca434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first tidyness issue: merge all dog columns into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c77e8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['dogs'] = df_clean['doggo'] + df_clean['floofer'] + df_clean['pupper'] + df_clean['puppo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f56a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['dogs'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e892c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.loc[df_clean['dogs'] == 'NoneNoneNoneNone',['dogs']] = 'None'\n",
    "df_clean.loc[df_clean['dogs'] == 'doggoNoneNoneNone',['dogs']] = 'doggo'\n",
    "df_clean.loc[df_clean['dogs'] == 'NoneNoneNonepuppo',['dogs']] = 'puppo'\n",
    "df_clean.loc[df_clean['dogs'] == 'NoneNonepupperNone',['dogs']] = 'pupper'\n",
    "df_clean.loc[df_clean['dogs'] == 'NoneflooferNoneNone',['dogs']] = 'floofer'\n",
    "df_clean.loc[df_clean['dogs'] == 'doggoNoneNonepuppo',['dogs']] = 'doggo, puppo'\n",
    "df_clean.loc[df_clean['dogs'] == 'doggoflooferNoneNone',['dogs']] = 'doggo, floofer'\n",
    "df_clean.loc[df_clean['dogs'] == 'doggoNonepupperNone',['dogs']] = 'doggo, pupper'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.drop(['puppo','pupper','floofer','doggo'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e06ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['dogs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second tidyness issue: merge all databases together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b69a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.merge(df_clean,df2_clean, on ='tweet_id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd03a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.merge(df_all,df3_clean, on='tweet_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f5a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cec8418",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_all.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca26c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.drop(['source_y','possibly_sensitive','possibly_sensitive_appealable','in_reply_to_status_id','in_reply_to_status_id_str','in_reply_to_user_id','in_reply_to_user_id_str','in_reply_to_screen_name','retweeted_status','quoted_status_id','quoted_status_id_str','quoted_status','is_quote_status'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98518cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_all.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f242bc9",
   "metadata": {},
   "source": [
    "## Test ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('Twitter_Archive_Master.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86369ab",
   "metadata": {},
   "source": [
    "# Insights #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e3992",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d749f2",
   "metadata": {},
   "source": [
    "Three insights gained from looking at the master database:\n",
    "\n",
    "    1- most denominators are 10 or above. Very few seem to fall below 10\n",
    "    2- the language used the most in english\n",
    "    3- there is a positive corralation between increasing likes and retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c51b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.rating_denominator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b5d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c244423",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,9))\n",
    "plt.scatter(df_all['favorite_count'],df_all['retweet_count'],color = \"blue\")\n",
    "plt.title('retweets vs favorites')\n",
    "plt.xlabel('Number of favorites')\n",
    "plt.ylabel('Number of retweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d37301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921c157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ae545e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
